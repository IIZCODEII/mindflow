{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37a606e",
   "metadata": {},
   "source": [
    "<div align='center'  ><img width=200px height=200px src=\"img_notebook/mindflow.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623c2b7",
   "metadata": {},
   "source": [
    "# MindFlow : Reconnaisance d'emotions en temps réel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c08b77",
   "metadata": {},
   "source": [
    "L'idée de notre projet est de proposer une application permettant de detecter les emotions cardinales en temps réel sur un flux vidéo webcam. L'objectif est d'apporter une contribution positive pour le diagnostic des problèmes de santé mentale sur les plateformes de streaming en ligne type Twitch.\n",
    "\n",
    "Dans cette visée d'être le plus utile et pratique possible, nous avons orienter notre travail sur la production d'une *proof-of-concept* concrète.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b08ca",
   "metadata": {},
   "source": [
    "## Concept et UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34fd260",
   "metadata": {},
   "source": [
    "![UI](img_notebook/mindflow_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0055d",
   "metadata": {},
   "source": [
    "La figure ci-dessus met en évidence l'interface utilisateur qui est née de cette recherche. Elle met en avant la necessité de ne pas se contenter des prédictions du modèle de reconnaissance et de restituer des statististiques *live* et sur l'ensemble de la session permettant de pointer vers un pré-diagnostic ( par exemple profil emotionel de la personne sur la session, part des émotions négatives etc).\n",
    "\n",
    "Pour materialiser cette ambition, il faut composer avec deux impératifs:\n",
    "\n",
    "* Bonne qualité des prédictions\n",
    "* Faible temps d'inference\n",
    "\n",
    "C'est avec ces contraintes en tête que nous avons articuler conceptuelement les différentes briques de MindFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e74e78",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1ff40",
   "metadata": {},
   "source": [
    "![conceptual_wf](img_notebook/video_feed_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6141f",
   "metadata": {},
   "source": [
    "### Face Detection Algorithm : Haar-Viola-Jones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4513b899",
   "metadata": {},
   "source": [
    "Choix d'une approche directe type Haar-Viola-Jones plutot que Deep Learning pour les raisons suivantes :\n",
    "\n",
    "* Temps de latence réduit\n",
    "* Pas de temps passé à optimiser et à entrainer le modèle\n",
    "* Utilisé et eprouvé dans l'industrie depuis des decennies sur les equipements photos et vidéos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ecfd6b",
   "metadata": {},
   "source": [
    "En particulier, l'algorithme de Haar-Viola-Jones permet de traiter au moins 15 images par secondes en moyenne et a été la première methode permettant la detection d'objet en temps réel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65abfb",
   "metadata": {},
   "source": [
    "![haar](img_notebook/img9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b34fc",
   "metadata": {},
   "source": [
    "* La méthode de Viola et Jones permet la reconnaissance faciale en discrétisant une image à l’aide d’une série de rectangles adjacents. Pour chacun d’eux, il faut calculer l’intensité moyenne des pixels qui s’y trouvent. On définit alors les caractéristiques pseudo-Haar, qui sont calculées par la différence des sommes de pixels de deux ou plusieurs zones rectangulaires adjacentes. \n",
    "\n",
    "* Nous allons procéder de façon similaire en changeant l’échelle de la décomposition de l’image et donc travailler sur des rectangles de plus en plus grands. Viola et Jones prennent ainsi un facteur multiplicatif de 1,25 à chaque changement d’échelle, jusqu'à ce que la fenêtre couvre la totalité de l'image.\n",
    "\n",
    "* Ils utilisent ensuite un algorithme de boosting, adapté d’AdaBoost.  Ce classificateur dit fort utilise la combinaison optimale de classificateurs faibles tels que des arbres de décision : il prend en compte à chaque nouvelle itération un nouveau classificateur faible. \n",
    "\n",
    "* Cette méthode risque d’induire une grande complexité du fait du fait des différents changements d’échelle. Viola et Jones introduisent la méthode dite des « cascades des classifieurs (ou classificateurs) » pour éviter les calculs inutiles et réduire la complexité. Ils veulent être capables d’identifier les zones où l’objet recherché ne se trouve vraisemblablement pas, les extraire du champ spatial d’investigation ai ainsi mieux se concentrer sur les parties de l’image où il a une probabilité non négligeable de se trouver. En  partant des classificateurs les plus simples lorsqu’on construit l’algorithme de boosting, on peut rejeter très rapidement la grande majorité des exemples négatifs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28973d12",
   "metadata": {},
   "source": [
    "## Face Emotion Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb3dcd",
   "metadata": {},
   "source": [
    "![iter](img_notebook/iter_fer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6860cf",
   "metadata": {},
   "source": [
    "Deux notebooks disctincts présent dans le dépot décrivent notre approche pour les deux itérations effectuées sur le modèle de FER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b76302",
   "metadata": {},
   "source": [
    "Principaux résultats de ces itérations:\n",
    "\n",
    "1. Acc ~56%, F1 Score 50% : prb de données => grayscale, definition trop basse, émotions caricaturales, methodes de scraping => label peu pertinent, beaucoup d'intersections, unbalanced data\n",
    "2. Acc ~62%, meilleure architecture et données cependant toujours les mêmes problemes de labelisations que sur la première iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd66978",
   "metadata": {},
   "source": [
    "Il est important de se rendre compte que ces métriques ne seront jamais completement revalatrices des vrais performances du FER model, l'inference se fait sur des données différentes que celles sur lesquelles le modèle est entrainé (stream webcam) et nous n'avons pas de quoi evaluer la performance sur les données strictes de notre cas d'usage.\n",
    "Les distributions sont différentes ( problème des datasets crowdsourcé/scrappé) meme si globalement les similarités permettent de rendre le FER model exploitable dans le cadre de cette POC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02923c6",
   "metadata": {},
   "source": [
    "## Workflow logiciel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1badc9dd",
   "metadata": {},
   "source": [
    "![iter](img_notebook/wf_mind.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbb867",
   "metadata": {},
   "source": [
    "## Lancer le projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2c95d",
   "metadata": {},
   "source": [
    "1. Télécharger le modèle DAN préentrainé ici https://drive.google.com/file/d/1uHNADViICyJEjJljv747nfvrGu12kjtu/view?usp=sharing et le mettre dans le répertoire app/models\n",
    "2. Build une image docker avec le dockerfile ou simplement installer les requierments via pip\n",
    "3. Executer la commande suivante à la racine du dépôt : `python app/server.py serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb6054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
